{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ1_xtISD2WL"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/5.classification/HW5_FeatureExploration.ipynb)\n",
        "\n",
        "**N.B.** Once it's open on Colab, remember to save a copy (by e.g. clicking `Copy to Drive` above).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o67Sahu3GWpc"
      },
      "source": [
        "# Feature engineering for text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZD-F6iB63lv"
      },
      "source": [
        "This notebook explores feature engineering for text classification.  Your task is to create two new feature functions (like `dictionary_feature` and `unigram_feature` below), and include them in the `build_features` function.  What features do you think will help for your particular problem? Your grade is *not* tied to whether accuracy goes up or down, so be creative!  You are free to read in any other external resources you like (dictionaries, document metadata, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajlr1qNk63lx"
      },
      "source": [
        "You are free to use any of the following datasets for this exercise, or to use your own (if you have your own labeled data with at least 500 examples from at least two classes, I would encourage you to use it!).  If you use your own data, just be sure to format it like the examples below; each directory has a `train.tsv`, `dev.tsv` and `test.tsv` file, where each file is tab-separated (label in the first column and text in the second column).\n",
        "\n",
        "* [Sentiment Analysis](https://ai.stanford.edu/~amaas/data/sentiment/) (Positive/Negative)\n",
        "* [Congressional Speech](https://www.cs.cornell.edu/home/llee/data/convote.html) (Democrat/Republican)\n",
        "* Library of Congress Subject Classication ([21 categories](https://en.wikipedia.org/wiki/Library_of_Congress_Classification))\n",
        "\n",
        "For whichever dataset you pick, download the data first using the code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HNh-QjKm67VB",
        "outputId": "d96104a4-4e20-4bdb-b77a-850434edfbf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-26 03:41:53--  https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/lmrd/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26961941 (26M) [text/plain]\n",
            "Saving to: ‘lmrd_train.tsv’\n",
            "\n",
            "lmrd_train.tsv      100%[===================>]  25.71M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-09-26 03:41:54 (174 MB/s) - ‘lmrd_train.tsv’ saved [26961941/26961941]\n",
            "\n",
            "--2025-09-26 03:41:54--  https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/lmrd/dev.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6693764 (6.4M) [text/plain]\n",
            "Saving to: ‘lmrd_dev.tsv’\n",
            "\n",
            "lmrd_dev.tsv        100%[===================>]   6.38M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-09-26 03:41:54 (72.4 MB/s) - ‘lmrd_dev.tsv’ saved [6693764/6693764]\n",
            "\n",
            "--2025-09-26 03:41:54--  https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/lmrd/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32863873 (31M) [text/plain]\n",
            "Saving to: ‘lmrd_test.tsv’\n",
            "\n",
            "lmrd_test.tsv       100%[===================>]  31.34M   151MB/s    in 0.2s    \n",
            "\n",
            "2025-09-26 03:41:55 (151 MB/s) - ‘lmrd_test.tsv’ saved [32863873/32863873]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get LMRD data\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/lmrd/train.tsv -O lmrd_train.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/lmrd/dev.tsv -O lmrd_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/lmrd/test.tsv -O lmrd_test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJd1C1cu7QD9"
      },
      "outputs": [],
      "source": [
        "# get Convote data\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/convote/train.tsv -O convote_train.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/convote/dev.tsv -O convote_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/convote/test.tsv -O convote_test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2kamLx07T6x"
      },
      "outputs": [],
      "source": [
        "# get LoC data\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/loc/train.tsv -O loc_train.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/loc/dev.tsv -O loc_dev.tsv\n",
        "!wget https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/loc/test.tsv -O loc_test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wh28crHR63ly",
        "outputId": "a04732fa-426c-4f6b-a43d-cd5822681225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import operator\n",
        "import sys\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from sklearn import linear_model, preprocessing\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaiV9Dy763lx"
      },
      "source": [
        "## Part 1: Loading data\n",
        "\n",
        "**Q1: Briefly describe your data (including the categories you're predicting).**  If you're using your own data, tell us about it; if you're using one of the datasets above, tell us something that shows you've looked at the data. How many examples are in each category?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8kDz__uf63lz"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):\n",
        "    df = pd.read_csv(filename, names=[\"label\", \"text\"], sep=\"\\t\")\n",
        "    return df.text.to_list(), df.label.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ej8Ge3bR63lz"
      },
      "outputs": [],
      "source": [
        "# Change this to the directory with the data you will be using.\n",
        "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
        "data = \"lmrd\"\n",
        "\n",
        "x_train, y_train = read_data(\"%s_train.tsv\" % data)\n",
        "x_dev, y_dev = read_data(\"%s_dev.tsv\" % data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TX79C75CGWpf",
        "outputId": "48dfad53-bdc5-4035-c92a-5a5d8849fddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 20000\n",
            "Dev set size: 5000\n",
            "Test set size: 25000\n",
            "\n",
            "Category counts in training set:\n",
            "label\n",
            "pos    10000\n",
            "neg    10000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample row:\n",
            "                                                 text label\n",
            "12  Ten out of ten stars is no exaggeration . This...   pos\n"
          ]
        }
      ],
      "source": [
        "# Switch to movie review dataset\n",
        "data = \"lmrd\"\n",
        "\n",
        "x_train, y_train = read_data(f\"{data}_train.tsv\")\n",
        "x_dev, y_dev = read_data(f\"{data}_dev.tsv\")\n",
        "x_test, y_test = read_data(f\"{data}_test.tsv\")\n",
        "\n",
        "# Convert to pandas for quick summary\n",
        "df_train = pd.DataFrame({\"text\": x_train, \"label\": y_train})\n",
        "\n",
        "print(\"Training set size:\", len(df_train))\n",
        "print(\"Dev set size:\", len(x_dev))\n",
        "print(\"Test set size:\", len(x_test))\n",
        "print(\"\\nCategory counts in training set:\")\n",
        "print(df_train[\"label\"].value_counts())\n",
        "\n",
        "print(\"\\nSample row:\")\n",
        "print(df_train.sample(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEbAvay5GWpf"
      },
      "source": [
        "## Part 2: Features\n",
        "\n",
        "Here, you will hand-engineer some features for your classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0l-v6WCl63l0"
      },
      "outputs": [],
      "source": [
        "## HELPER FUNCTIONS ##\n",
        "\n",
        "def majority_class(y_train, y_dev):\n",
        "    label_counts = Counter(y_train)\n",
        "    majority = label_counts.most_common(1)[0][0]\n",
        "\n",
        "    correct = 0.\n",
        "    for label in y_dev:\n",
        "        if label == majority:\n",
        "            correct += 1\n",
        "\n",
        "    print(\"%s\\t%.3f\" % (majority, correct/len(y_dev)))\n",
        "    return correct / len(y_dev)\n",
        "\n",
        "def build_features(x_train, feature_functions):\n",
        "    data = []\n",
        "    for doc in x_train:\n",
        "        feats = {}\n",
        "        tokens = doc.split(\" \")\n",
        "\n",
        "        for function in feature_functions:\n",
        "            feats.update(function(tokens))\n",
        "\n",
        "        data.append(feats)\n",
        "    return data\n",
        "\n",
        "# This helper function converts a dictionary of feature names to unique numerical ids\n",
        "def create_vocab(data):\n",
        "    feature_vocab = {}\n",
        "    idx = 0\n",
        "    for doc in data:\n",
        "        for feat in doc:\n",
        "            if feat not in feature_vocab:\n",
        "                feature_vocab[feat] = idx\n",
        "                idx += 1\n",
        "\n",
        "    return feature_vocab\n",
        "\n",
        "# This helper function converts a dictionary of feature names to a sparse representation\n",
        "# that we can fit in a scikit-learn model.  This is important because almost all feature\n",
        "# values will be 0 for most documents (note: why?), and we don't want to save them all in\n",
        "# memory.\n",
        "\n",
        "def features_to_ids(data, feature_vocab):\n",
        "    new_data = sparse.lil_matrix((len(data), len(feature_vocab)))\n",
        "    for idx,doc in enumerate(data):\n",
        "        for f in doc:\n",
        "            if f in feature_vocab:\n",
        "                new_data[idx, feature_vocab[f]] = doc[f]\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPtsXae_63l0"
      },
      "source": [
        "We'll start with two feature classes -- one feature class noting the presence of a word in an external dictionary, and one feature class for the word identity (i.e., unigram).  We'll implement each feature class as a function that takes a single document as input (as a list of tokens) and returns a dict corresponding to the feature we're creating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m7wpOrJ563l1"
      },
      "outputs": [],
      "source": [
        "# Here's a sample dictionary we can create by inspecting the output of the Mann-Whitney test (in 2.compare/)\n",
        "\n",
        "# EDIT TO FIT YOUR DATASET\n",
        "dem_dictionary = set([\"republican\",\"cut\", \"opposition\"])\n",
        "repub_dictionary = set([\"growth\",\"economy\"])\n",
        "\n",
        "def political_dictionary_feature(tokens):\n",
        "    feats = {}\n",
        "    for word in tokens:\n",
        "        if word in dem_dictionary:\n",
        "            feats[\"word_in_dem_dictionary\"] = 1\n",
        "        if word in repub_dictionary:\n",
        "            feats[\"word_in_repub_dictionary\"] = 1\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wtO-foMj63l1"
      },
      "outputs": [],
      "source": [
        "def unigram_feature(tokens):\n",
        "    feats = {}\n",
        "    for word in tokens:\n",
        "        feats[\"UNIGRAM_%s\" % word] = 1\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7yZfUkU63l1"
      },
      "source": [
        "**Q2**: **Add first new feature function here.**  Describe your feature and why you think it will help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PAw7PO_w63l1"
      },
      "outputs": [],
      "source": [
        "def new_feature_class_one(tokens):\n",
        "    feats = {}\n",
        "    # count how many tokens contain \"!\"\n",
        "    feats[\"exclaim_count\"] = sum(1 for t in tokens if \"!\" in t)\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My first feature captures emphasis in writing, specifically by counting exclamation marks (and/or ALL CAPS words). I think this will help because people often use punctuation and capitalization to intensify sentiment in reviews. For example, a positive review might say “Amazing movie!!!” while a negative one might say “AWFUL!!!”. These stylistic cues add another layer of information beyond just the words themselves, so they should provide the classifier with useful signals about the strength and direction of sentiment."
      ],
      "metadata": {
        "id": "sAslGVPQKa5N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GLI_RtJ63l2"
      },
      "source": [
        "**Q3**: **Add second new feature function here.** Describe your feature and why you think it will help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jNcqQIx863l2"
      },
      "outputs": [],
      "source": [
        "# tiny sentiment lexicon\n",
        "pos_words = set([\"good\",\"great\",\"amazing\",\"wonderful\",\"excellent\",\"best\",\"love\"])\n",
        "neg_words = set([\"bad\",\"terrible\",\"awful\",\"worst\",\"boring\",\"hate\",\"poor\"])\n",
        "\n",
        "def new_feature_class_two(tokens):\n",
        "    feats = {}\n",
        "    feats[\"pos_word_count\"] = sum(1 for t in tokens if t.lower() in pos_words)\n",
        "    feats[\"neg_word_count\"] = sum(1 for t in tokens if t.lower() in neg_words)\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My second feature counts the number of clearly positive and negative words in a review, based on a small sentiment lexicon (e.g., words like great, amazing, awful, boring). I think this will help because movie reviews often include very explicit polarity words, and simply counting them gives the classifier a direct signal of sentiment. For example, a review with many positive words is more likely to be labeled positive, and vice versa for negative words."
      ],
      "metadata": {
        "id": "TYjpUOEaKMww"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07dK4NPp63l2"
      },
      "source": [
        "We use the `build_features` helper function to aggregate together all of the information from different feature classes.  Each document has a feature dict (`feats`), and we'll update that dict with the new dict that each separate feature class is returning.  (Here you want to make sure that the keys each feature function is creating are unique so they don't get clobbered by other functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RMh8GzFm63l2"
      },
      "outputs": [],
      "source": [
        "# This function trains a model and returns the predicted and true labels for test data\n",
        "def evaluate(x_train, x_dev, y_train, y_dev, feature_functions):\n",
        "    x_train_feat = build_features(x_train, feature_functions)\n",
        "    x_dev_feat = build_features(x_dev, feature_functions)\n",
        "\n",
        "    # just create vocabulary from features in *training* data\n",
        "    feature_vocab = create_vocab(x_train_feat)\n",
        "\n",
        "    x_train_ids = features_to_ids(x_train_feat, feature_vocab)\n",
        "    x_dev_ids = features_to_ids(x_dev_feat, feature_vocab)\n",
        "\n",
        "    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n",
        "    logreg.fit(x_train_ids, y_train)\n",
        "    predictions = logreg.predict(x_dev_ids)\n",
        "    return (predictions, y_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zH7bPWTA63l3"
      },
      "outputs": [],
      "source": [
        "def print_weights(clf, vocab, n=10):\n",
        "    reverse_vocab = [None]*len(clf.coef_[0])\n",
        "    for k in vocab:\n",
        "        reverse_vocab[vocab[k]] = k\n",
        "\n",
        "    if len(clf.classes_) == 2:\n",
        "\n",
        "        weights=clf.coef_[0]\n",
        "        for feature, weight in sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))[:n]:\n",
        "            print(\"%.3f\\t%s\" % (weight, feature))\n",
        "\n",
        "        print()\n",
        "\n",
        "        for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "            print(\"%.3f\\t%s\" % (weight, feature))\n",
        "\n",
        "    else:\n",
        "        for i, cat in enumerate(clf.classes_):\n",
        "\n",
        "            weights=clf.coef_[i]\n",
        "\n",
        "            for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "            print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CFfW35rB63l3",
        "outputId": "eeba6cbd-bcb3-469c-efaa-62a2e90d3480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos\t0.500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "majority_class(y_train,y_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs9jWjaT63l3"
      },
      "source": [
        "Explore the impact of different feature functions by evaluating them below:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigram feature: count lowercase unigrams\n",
        "def unigram_feature(tokens):\n",
        "    feats = {}\n",
        "    for w in tokens:\n",
        "        w = w.strip()\n",
        "        if not w:\n",
        "            continue\n",
        "        key = f\"UNI={w.lower()}\"\n",
        "        feats[key] = feats.get(key, 0) + 1\n",
        "    return feats"
      ],
      "metadata": {
        "id": "4AnyyXbEPUrv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic training/eval pipeline that mirrors `evaluate` but returns the trained clf + vocab\n",
        "from sklearn import linear_model\n",
        "import numpy as np\n",
        "\n",
        "def pipeline(x_train, x_dev, y_train, y_dev, feature_functions):\n",
        "    # Build feature dicts\n",
        "    Xtr_feats = build_features(x_train, feature_functions)\n",
        "    Xdv_feats = build_features(x_dev,   feature_functions)\n",
        "\n",
        "    # Fit vocab on train only\n",
        "    vocab = create_vocab(Xtr_feats)\n",
        "\n",
        "    # Convert to sparse matrices\n",
        "    Xtr = features_to_ids(Xtr_feats, vocab)\n",
        "    Xdv = features_to_ids(Xdv_feats, vocab)\n",
        "\n",
        "    # Train simple logistic regression\n",
        "    clf = linear_model.LogisticRegression(\n",
        "        C=1.0, solver='lbfgs', penalty='l2', max_iter=10000\n",
        "    )\n",
        "    clf.fit(Xtr, y_train)\n",
        "\n",
        "    # Report dev accuracy\n",
        "    preds = clf.predict(Xdv)\n",
        "    acc = np.mean(preds == y_dev)\n",
        "    print(f\"Dev accuracy: {acc:.3f}\")\n",
        "\n",
        "    return clf, vocab\n"
      ],
      "metadata": {
        "id": "z9vn5QRBPUi_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "u2ci2NKi63l3",
        "outputId": "c0086559-a127-4699-f53e-eef01a925289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev accuracy: 0.888\n",
            "-2.443\tUNI=4/10\n",
            "-2.284\tUNI=waste\n",
            "-2.227\tUNI=worst\n",
            "-1.706\tUNI=poorly\n",
            "-1.653\tUNI=disappointment\n",
            "-1.614\tUNI=awful\n",
            "-1.556\tUNI=boring\n",
            "-1.515\tUNI=3/10\n",
            "-1.461\tUNI=*1/2\n",
            "-1.458\tUNI=lacks\n",
            "\n",
            "2.371\tUNI=7/10\n",
            "1.615\tUNI=wonderfully\n",
            "1.427\tUNI=refreshing\n",
            "1.408\tUNI=8/10\n",
            "1.346\tUNI=7\n",
            "1.331\tUNI=excellent\n",
            "1.310\tUNI=perfect\n",
            "1.270\tUNI=surprisingly\n",
            "1.234\tUNI=subtle\n",
            "1.231\tUNI=8\n",
            "Dev accuracy: 0.501\n",
            "-0.010\texclaim_count\n",
            "\n",
            "-0.010\texclaim_count\n",
            "Dev accuracy: 0.700\n",
            "-1.095\tneg_word_count\n",
            "0.420\tpos_word_count\n",
            "\n",
            "0.420\tpos_word_count\n",
            "-1.095\tneg_word_count\n",
            "Dev accuracy: 0.700\n",
            "-1.095\tneg_word_count\n",
            "0.000\texclaim_count\n",
            "0.420\tpos_word_count\n",
            "\n",
            "0.420\tpos_word_count\n",
            "0.000\texclaim_count\n",
            "-1.095\tneg_word_count\n",
            "Dev accuracy: 0.887\n",
            "-2.434\tUNI=4/10\n",
            "-2.274\tUNI=waste\n",
            "-1.685\tUNI=poorly\n",
            "-1.643\tUNI=disappointment\n",
            "-1.510\tUNI=3/10\n",
            "-1.459\tUNI=lacks\n",
            "-1.446\tUNI=*1/2\n",
            "-1.353\tUNI=avoid\n",
            "-1.341\tUNI=mst3k\n",
            "-1.338\tUNI=disappointing\n",
            "\n",
            "2.355\tUNI=7/10\n",
            "1.604\tUNI=wonderfully\n",
            "1.416\tUNI=refreshing\n",
            "1.415\tUNI=8/10\n",
            "1.345\tUNI=7\n",
            "1.314\tUNI=perfect\n",
            "1.259\tUNI=surprisingly\n",
            "1.242\tUNI=subtle\n",
            "1.236\tUNI=8\n",
            "1.183\tUNI=sensitive\n"
          ]
        }
      ],
      "source": [
        "# 1) Unigram baseline\n",
        "features = [unigram_feature]\n",
        "clf, vocab = pipeline(x_train, x_dev, y_train, y_dev, features)\n",
        "print_weights(clf, vocab, n=10)\n",
        "\n",
        "# 2) First feature only\n",
        "features = [new_feature_class_one]\n",
        "clf, vocab = pipeline(x_train, x_dev, y_train, y_dev, features)\n",
        "print_weights(clf, vocab, n=10)\n",
        "\n",
        "# 3) Second feature only\n",
        "features = [new_feature_class_two]\n",
        "clf, vocab = pipeline(x_train, x_dev, y_train, y_dev, features)\n",
        "print_weights(clf, vocab, n=10)\n",
        "\n",
        "# 4) Both new features\n",
        "features = [new_feature_class_one, new_feature_class_two]\n",
        "clf, vocab = pipeline(x_train, x_dev, y_train, y_dev, features)\n",
        "print_weights(clf, vocab, n=10)\n",
        "\n",
        "# 5) Unigrams + both new features (usually strongest)\n",
        "features = [unigram_feature, new_feature_class_one, new_feature_class_two]\n",
        "clf, vocab = pipeline(x_train, x_dev, y_train, y_dev, features)\n",
        "print_weights(clf, vocab, n=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXsGfRng63l3"
      },
      "source": [
        "If you want to print the coefficients for any of the models you train, you can do so like this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Pzzj7s4Q63l3",
        "outputId": "7291bc60-8b5b-461e-bd07-153122107a00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-2.434\tUNI=4/10\n",
            "-2.274\tUNI=waste\n",
            "-1.685\tUNI=poorly\n",
            "-1.643\tUNI=disappointment\n",
            "-1.510\tUNI=3/10\n",
            "-1.459\tUNI=lacks\n",
            "-1.446\tUNI=*1/2\n",
            "-1.353\tUNI=avoid\n",
            "-1.341\tUNI=mst3k\n",
            "-1.338\tUNI=disappointing\n",
            "\n",
            "2.355\tUNI=7/10\n",
            "1.604\tUNI=wonderfully\n",
            "1.416\tUNI=refreshing\n",
            "1.415\tUNI=8/10\n",
            "1.345\tUNI=7\n",
            "1.314\tUNI=perfect\n",
            "1.259\tUNI=surprisingly\n",
            "1.242\tUNI=subtle\n",
            "1.236\tUNI=8\n",
            "1.183\tUNI=sensitive\n"
          ]
        }
      ],
      "source": [
        "print_weights(clf, vocab, n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coefficient Interpretation:\n",
        "Looking at the learned weights, the model strongly associates words like “poorly,” “awful,” “boring,” “waste” with the negative class, and words like “excellent,” “wonderfully,” “refreshing,” “perfect” with the positive class. Numeric ratings such as “7/10” and “8/10” also appear as highly positive indicators, while “3/10” and “4/10” are strong negatives. My hand-engineered lexicon features show up with sensible weights too: pos_word_count is positive and neg_word_count is negative, which matches intuition. This suggests the model is learning both explicit polarity words and stylistic cues from reviews, and that the extra features reinforce those patterns."
      ],
      "metadata": {
        "id": "GJkxUH8VQli3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVwYUGla63l4"
      },
      "source": [
        "## Part 3: Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQib8YEGGWph"
      },
      "source": [
        "**Q4**: Implement a function that returns the parametric confidence interval bounds for a binomial estimator of the model accuracy. It should return a tuple of floats `(lower_bound, upper_bound)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "V4V_h2MFGWph"
      },
      "outputs": [],
      "source": [
        "from statistics import NormalDist\n",
        "import numpy as np\n",
        "\n",
        "def binomial_test(predictions, targets, significance_level=0.95):\n",
        "    \"\"\"\n",
        "    Returns (lower_bound, upper_bound) for the accuracy (binomial proportion).\n",
        "    Uses the Wilson score interval, which is more reliable than the simple Wald CI.\n",
        "    \"\"\"\n",
        "    preds = np.array(predictions)\n",
        "    gold  = np.array(targets)\n",
        "    n = len(gold)\n",
        "    if n == 0:\n",
        "        return (0.0, 0.0)\n",
        "\n",
        "    # accuracy as binomial proportion\n",
        "    p_hat = (preds == gold).mean()\n",
        "\n",
        "    # two-sided z for the requested confidence level\n",
        "    z = NormalDist().inv_cdf((1.0 + significance_level) / 2.0)\n",
        "\n",
        "    denom = 1.0 + (z**2)/n\n",
        "    center = (p_hat + (z**2)/(2*n)) / denom\n",
        "    half_width = (z * np.sqrt((p_hat*(1 - p_hat)/n) + (z**2)/(4*n**2))) / denom\n",
        "\n",
        "    lower = max(0.0, center - half_width)\n",
        "    upper = min(1.0, center + half_width)\n",
        "    return (float(lower), float(upper))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds, gold = evaluate(x_train, x_dev, y_train, y_dev, [unigram_feature, new_feature_class_one, new_feature_class_two])\n",
        "ci = binomial_test(preds, gold, significance_level=0.95)\n",
        "print(\"95% CI for accuracy:\", ci)"
      ],
      "metadata": {
        "id": "ksjR7-wWRBs-",
        "outputId": "282df80b-a3e2-4721-d2f7-b40cd0c1b989",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95% CI for accuracy: (0.8781325107620659, 0.8956729808516819)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmz4Pv5CGWph"
      },
      "source": [
        "**Q5**: Plot the performance for models trained with different combinations of features, including your two custom features. Some combinations you might try (but feel free to pick your own!):\n",
        "1. Just the dictionary features\n",
        "2. Just the unigram features\n",
        "3. Just your custom features\n",
        "4. Unigram features + custom features\n",
        "\n",
        "Make a bar plot with confidence intervals. Does incorporating your features result in a statistically significant change in performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDKnZ8V263l3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the feature sets to compare\n",
        "feature_sets = {\n",
        "    \"Unigrams only\": [unigram_feature],\n",
        "    \"Custom1 only\": [new_feature_class_one],\n",
        "    \"Custom2 only\": [new_feature_class_two],\n",
        "    \"Both custom\": [new_feature_class_one, new_feature_class_two],\n",
        "    \"Unigrams + Custom\": [unigram_feature, new_feature_class_one, new_feature_class_two],\n",
        "    \"Dictionary + Unigrams\": [political_dictionary_feature, unigram_feature]\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, feats in feature_sets.items():\n",
        "    preds, gold = evaluate(x_train, x_dev, y_train, y_dev, feats)\n",
        "    acc = (np.array(preds) == np.array(gold)).mean()\n",
        "    lb, ub = binomial_test(preds, gold, significance_level=0.95)\n",
        "    results.append((name, acc, lb, ub))\n",
        "\n",
        "# Convert to DataFrame for easy plotting\n",
        "df_results = pd.DataFrame(results, columns=[\"Model\",\"Accuracy\",\"Lower\",\"Upper\"])\n",
        "print(df_results)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "x = np.arange(len(df_results))\n",
        "y = df_results[\"Accuracy\"]\n",
        "err_lower = df_results[\"Accuracy\"] - df_results[\"Lower\"]\n",
        "err_upper = df_results[\"Upper\"] - df_results[\"Accuracy\"]\n",
        "\n",
        "plt.bar(x, y, yerr=[err_lower, err_upper], capsize=5, alpha=0.7)\n",
        "plt.xticks(x, df_results[\"Model\"], rotation=30, ha=\"right\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy with 95% CI\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFfAJs8FEDc-"
      },
      "source": [
        "---\n",
        "\n",
        "## To submit\n",
        "\n",
        "Congratulations on finishing this homework!\n",
        "Please follow the instructions below to download the notebook file (`.ipynb`) and its printed version (`.pdf`) for submission on bCourses -- remember **all cells must be executed**.\n",
        "\n",
        "1.  Download a copy of the notebook file: `File > Download > Download .ipynb`.\n",
        "\n",
        "2.  Print the notebook as PDF (via your browser, or tools like [nbconvert](https://nbconvert.readthedocs.io/en/latest/))."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}