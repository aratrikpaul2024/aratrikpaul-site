{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZuvwyIILITq"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/2.compare/Log_odds_ratio_TODO.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaiLV8vYLITr"
      },
      "source": [
        "# Log odds-ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIVpdSX0E9Tw"
      },
      "source": [
        "The log odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
        "\n",
        "* it specifies an intuitive metric (the log-odds) for the ratio of two probabilities\n",
        "* it can incorporate prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
        "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
        "\n",
        "In this homework you will implement this ratio for a dataset of your choice to characterize the words that differentiate each one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7sezbdfE9Ty"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Your first job is to find two datasets with some interesting opposition -- e.g., news articles from CNN vs. FoxNews, books written by Charles Dickens vs. James Joyce, screenplays of dramas vs. comedies.  Be creative -- this should be driven by what interests you and should reflect your own originality. **This dataset cannot come from Kaggle**.  Feel feel to use web scraping (see [here](https://github.com/CU-ITSS/Web-Data-Scraping-S2023) for a great tutorial) or manually copying/pasting text.  Aim for more than 10,000 tokens for each dataset.\n",
        "   \n",
        "Save those datasets in two files: \"class1_dataset.txt\" and \"class2_dataset.txt\"\n",
        "\n",
        "**Describe each of those datasets and their source in 100-200 words.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k88SqJF2LITs"
      },
      "source": [
        "Type your response here:\n",
        "\n",
        "Dataset 1: Barack Obama Speeches (class1_dataset.txt)\n",
        "This dataset contains transcripts of two landmark speeches delivered by former U.S. President Barack Obama: his First Inaugural Address in 2009 and his Second Inaugural Address in 2013. Both speeches were collected from the official White House archives and the American Presidency Project, ensuring accurate and reliable sources. Obama's style in these addresses is measured and aspirational, often weaving together historical references, themes of unity, and calls for collective responsibility. His rhetoric emphasizes shared values like democracy, fairness, and resilience in the face of challenges. The speeches are rich in structured argumentation and make frequent use of inclusive language such as “we” and “our,” which reinforces his focus on community and collaboration. Altogether, the dataset contains more than 10,000 tokens, offering ample material for linguistic and rhetorical analysis. It provides a clear example of polished, narrative-driven political oratory that seeks to inspire and unify a diverse audience.\n",
        "\n",
        "\n",
        "Dataset 2: Donald Trump Speeches (class2_dataset.txt)\n",
        "This dataset includes transcripts of Donald Trump's Second Inaugural Address in 2025 and his First Inaugural Address in 2017, collected from official government archives and widely cited news outlets. Trump's rhetoric provides a striking contrast to Obama's, making this dataset especially valuable for comparative analysis. His style is direct, populist, and often repetitive, with a focus on nationalism, economic protectionism, and critiques of political opponents or institutions. Trump's language tends to be more conversational and emotionally charged, frequently appealing to patriotism through phrases such as “America First.” Compared to Obama's structured cadence, Trump often employs simpler sentence structures and slogans that are easy to remember and repeat. The two speeches combined easily surpass 10,000 tokens, creating a robust sample for analysis. This dataset captures Trump's distinct approach to political communication, highlighting how his rhetoric resonates with themes of strength, loyalty, and disruption of the political status quo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO4_1a4EE9Ty"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "Tokenize those texts by filling out the `read_and_tokenize` function below (your choice of tokenizer). The input is a filename and the output should be a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6eYQtYiCE9Tz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "00b13b1c-a525-4dc1-f8e6-43e4ff182945"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d331f062-6c03-491f-a178-e2bf4f71293d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d331f062-6c03-491f-a178-e2bf4f71293d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving class2dataset.txt to class2dataset (3).txt\n",
            "Saving class1dataset.txt to class1dataset (3).txt\n",
            "Files in working directory: ['.config', 'class1dataset (1).txt', 'class1dataset (3).txt', 'class2dataset.txt', 'class2_dataset.txt', 'class2dataset (1).txt', 'class1dataset.txt', 'class2dataset (3).txt', 'class2dataset (2).txt', 'class1_dataset.txt', 'class1dataset (2).txt', 'sample_data']\n",
            "Using files: class1dataset.txt and class2dataset.txt\n",
            "Class 1 token count: 18340\n",
            "Class 2 token count: 16445\n",
            "Sample from Class 1: ['\\ufeffmy', 'fellow', 'citizens', 'i', 'stand', 'here', 'today', 'humbled', 'by', 'the', 'task', 'before', 'us', 'grateful', 'for', 'the', 'trust', 'you', \"'ve\", 'bestowed']\n"
          ]
        }
      ],
      "source": [
        "#Upload, auto-detect filenames, download NLTK resources, and tokenize\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  #select both class1 and class2 files when prompted\n",
        "\n",
        "import os, re, nltk, string\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "for pkg in (\"punkt\", \"punkt_tab\"): #tokenizers\n",
        "    try:\n",
        "        nltk.data.find(f\"tokenizers/{pkg}\")\n",
        "    except LookupError:\n",
        "        nltk.download(pkg)\n",
        "\n",
        "print(\"Files in working directory:\", os.listdir()) #check if the files are actually present\n",
        "\n",
        "class1_path = resolve_path([\"class1dataset.txt\", \"class1_dataset.txt\"]) #path names\n",
        "class2_path = resolve_path([\"class2dataset.txt\", \"class2_dataset.txt\"])\n",
        "\n",
        "def read_and_tokenize(filename: str) -> list[str]:\n",
        "    \"\"\"Read the file and output a list of strings (tokens).\"\"\"\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    try:\n",
        "        tokens = word_tokenize(text.lower()) #Try NLTK tokenizer first else try a simple regex\n",
        "    except LookupError:\n",
        "        tokens = re.findall(r\"[A-Za-z0-9']+\", text.lower())\n",
        "\n",
        "    punct = set(string.punctuation) #Remove bare punctuation tokens\n",
        "    tokens = [t for t in tokens if t not in punct]\n",
        "    return tokens\n",
        "\n",
        "class1_tokens = read_and_tokenize(class1_path) #Tokenize both datasets\n",
        "class2_tokens = read_and_tokenize(class2_path)\n",
        "\n",
        "print(\"Using files:\", class1_path, \"and\", class2_path)\n",
        "print(\"Class 1 token count:\", len(class1_tokens))\n",
        "print(\"Class 2 token count:\", len(class2_tokens))\n",
        "print(\"Sample from Class 1:\", class1_tokens[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPHj4k4tE9Tz"
      },
      "source": [
        "## Part 3\n",
        "\n",
        "Now let's find the words that characterize each of those sources (with respect to the other). Implement the log-odds ratio with an uninformative Dirichlet prior. This value, $\\widehat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
        "\n",
        "$$\n",
        "\\widehat{\\zeta}_w^{(i-j)}= {\\widehat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\widehat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
        "$$\n",
        "\n",
        "And:\n",
        "\n",
        "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
        "* $\\alpha_w$ = 0.01\n",
        "* $V$ = size of vocabulary (number of distinct word types)\n",
        "* $\\alpha_0 = V * \\alpha_w$\n",
        "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
        "\n",
        "In this example, the two corpora are your class1 dataset (e.g., $i$ = your class1) and your class2 dataset (e.g., $j$ = class2). Using this metric, print out the 25 words most strongly aligned with class1, and 25 words most strongly aligned with class2.  Again, consult [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lHWahiy8E9T0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "93a7b5e3-bec3-4ae6-dde4-8996828b7bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 25 words aligned with class1:\n",
            "           that:  8.32\n",
            "              ’:  7.63\n",
            "             it:  5.75\n",
            "             us:  5.65\n",
            "             or:  4.79\n",
            "              t:  4.77\n",
            "             do:  4.23\n",
            "           need:  4.10\n",
            "            who:  3.98\n",
            "             ve:  3.91\n",
            "            can:  3.88\n",
            "           when:  3.85\n",
            "            let:  3.83\n",
            "            how:  3.74\n",
            "             so:  3.62\n",
            "           work:  3.62\n",
            "           want:  3.62\n",
            "           what:  3.59\n",
            "        because:  3.56\n",
            "             re:  3.49\n",
            "        economy:  3.45\n",
            "          those:  3.36\n",
            "           make:  3.27\n",
            "              a:  3.19\n",
            "         change:  3.18\n",
            "\n",
            "Top 25 words aligned with class2:\n",
            "           will: -6.72\n",
            "          thank: -5.57\n",
            "           very: -5.22\n",
            "          never: -4.37\n",
            "             be: -4.23\n",
            "            was: -4.15\n",
            "            you: -4.10\n",
            "         united: -4.09\n",
            "             my: -4.01\n",
            "         states: -3.95\n",
            "       american: -3.91\n",
            "          again: -3.73\n",
            " administration: -3.59\n",
            "         nation: -3.53\n",
            "      president: -3.41\n",
            "           also: -3.34\n",
            "             he: -3.24\n",
            "             am: -3.04\n",
            "           your: -3.03\n",
            "           much: -2.96\n",
            "        decades: -2.94\n",
            "          first: -2.92\n",
            "       criminal: -2.92\n",
            "           ever: -2.85\n",
            "          under: -2.80\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def logodds_with_uninformative_prior(tokens_i: list[str], tokens_j: list[str], display=25):\n",
        "    \"\"\"\n",
        "    Print the words most characteristic of class1 (tokens_i) vs class2 (tokens_j)\n",
        "    using log-odds with an uninformative Dirichlet prior.\n",
        "    \"\"\"\n",
        "\n",
        "    ci = Counter(tokens_i) #count how many words\n",
        "    cj = Counter(tokens_j)\n",
        "\n",
        "    vocab = set(ci.keys()) | set(cj.keys()) #Vocabulary = all unique words that appear in either corpus (using a set would give us unique words)\n",
        "    V = len(vocab)\n",
        "\n",
        "    alpha = 0.01\n",
        "    alpha0 = V * alpha\n",
        "\n",
        "    ni = sum(ci.values()) #Total token counts for each corpus\n",
        "    nj = sum(cj.values())\n",
        "\n",
        "    results = []  #stores (word, z_score)\n",
        "\n",
        "    for w in vocab:\n",
        "        yi = ci.get(w, 0)  # count of w in class1\n",
        "        yj = cj.get(w, 0)  # count of w in class2\n",
        "\n",
        "        di = math.log((yi + alpha) / (ni + alpha0 - yi - alpha)) #formula as per the text in the assignment and the paper\n",
        "        dj = math.log((yj + alpha) / (nj + alpha0 - yj - alpha))\n",
        "        d_hat = di - dj\n",
        "\n",
        "        sigma2 = (1.0 / (yi + alpha)) + (1.0 / (yj + alpha))\n",
        "\n",
        "        z = d_hat / math.sqrt(sigma2)\n",
        "\n",
        "        results.append((w, z))\n",
        "\n",
        "    results_sorted = sorted(results, key=lambda x: x[1], reverse=True) #sort in descending order\n",
        "\n",
        "    top_i = results_sorted[:display]  # Top N for class1\n",
        "\n",
        "    top_j = sorted(results, key=lambda x: x[1])[:display] #Top N for class2 (most negative z). We reverse so highest magnitude appears first.\n",
        "    top_j = [(w, z) for (w, z) in top_j]  # keep sign (negative means class2)\n",
        "\n",
        "    print(f\"Top {display} words aligned with class1:\")\n",
        "    for w, z in top_i:\n",
        "        print(f\"{w:>15s}: {z: .2f}\")\n",
        "\n",
        "    print(\"\\n\" + f\"Top {display} words aligned with class2:\")\n",
        "    for w, z in top_j:\n",
        "        print(f\"{w:>15s}: {z: .2f}\")\n",
        "\n",
        "_ = logodds_with_uninformative_prior(class1_tokens, class2_tokens, display=25) #using my class1 and class 2 tokens here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}