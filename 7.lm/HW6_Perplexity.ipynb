{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d45605ec-3f07-40d8-ab31-10a38ce83b79",
      "metadata": {
        "id": "d45605ec-3f07-40d8-ab31-10a38ce83b79"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/7.lm/HW6_Perplexity.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "397af444-94c2-477b-92b8-4467b5ea80af",
      "metadata": {
        "id": "397af444-94c2-477b-92b8-4467b5ea80af"
      },
      "source": [
        "# HW6: Perplexity\n",
        "\n",
        "In this homework, you will implement a function to calculate the perplexity of the n-gram language models we covered in lab, and experiment with different sequences to better understand both n-gram LMs as well as the perplexity metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b52b3a60",
      "metadata": {
        "id": "b52b3a60",
        "outputId": "27ef5b89-7f18-4d41-9303-eb2dccff7889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import copy\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "33eb376a",
      "metadata": {
        "id": "33eb376a"
      },
      "outputs": [],
      "source": [
        "def read_file(filename):\n",
        "    sequences = []\n",
        "    with open(filename) as file:\n",
        "        data = file.read()\n",
        "        sents = sent_tokenize(data)\n",
        "        for sent in sents:\n",
        "            tokens = word_tokenize(sent)\n",
        "            sequences.append(tokens)\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a66e06bf-948f-4323-9af8-5864b466b033",
      "metadata": {
        "id": "a66e06bf-948f-4323-9af8-5864b466b033",
        "outputId": "4d32deea-47be-4f89-98f1-d16c42452ff5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-03 03:19:21--  https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/1342_pride_and_prejudice.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 691804 (676K) [text/plain]\n",
            "Saving to: ‘1342_pride_and_prejudice.txt’\n",
            "\n",
            "\r          1342_prid   0%[                    ]       0  --.-KB/s               \r1342_pride_and_prej 100%[===================>] 675.59K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2025-10-03 03:19:21 (142 MB/s) - ‘1342_pride_and_prejudice.txt’ saved [691804/691804]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/refs/heads/main/data/1342_pride_and_prejudice.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "feeb46f4",
      "metadata": {
        "id": "feeb46f4"
      },
      "outputs": [],
      "source": [
        "# Read data from file and tokenize them into sequences comprised of tokens.\n",
        "\n",
        "# Pride and Prejudice (Jane Austen)\n",
        "sequences = read_file(\"1342_pride_and_prejudice.txt\")\n",
        "\n",
        "max_sequences = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "214dbd04",
      "metadata": {
        "id": "214dbd04"
      },
      "outputs": [],
      "source": [
        "class NgramModel():\n",
        "\n",
        "    def __init__(self, sequences, order):\n",
        "\n",
        "        # For this exercise we're going to encode the LM as a sparse dictionary (trading less storage for more compute)\n",
        "        # We'll store the LM as a dictionary with the conditioning context as keys; each value is a\n",
        "        # Counter object that keeps track of the number of times we see a word following that context.\n",
        "        self.counts = {}\n",
        "\n",
        "        # Markov order (order 1 = conditioning on previous 1 word; order 2 = previous 2 words, etc.)\n",
        "        self.order = order\n",
        "\n",
        "        vocab = {\"[END]\": 0}\n",
        "\n",
        "        for s_idx, tokens in enumerate(sequences):\n",
        "            # We'll add [START] and [END] tokens to encode the beginning/end of sentences\n",
        "            tokens = [\"[START]\"] * order + tokens + [\"[END]\"]\n",
        "\n",
        "            if s_idx == 0:\n",
        "                print(tokens)\n",
        "\n",
        "            for i in range(order, len(tokens)):\n",
        "                context = \" \".join(tokens[i - order:i])\n",
        "                word = tokens[i]\n",
        "\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = len(vocab)\n",
        "\n",
        "                # For just the first sentence, print the conditioning context + word\n",
        "                if s_idx == 0:\n",
        "                    print(\"Context: %s Next: %s\" % (context.ljust(50), word))\n",
        "\n",
        "                if context not in self.counts:\n",
        "                    self.counts[context] = Counter()\n",
        "                self.counts[context][word] += 1\n",
        "\n",
        "\n",
        "\n",
        "    def sample(self, context):\n",
        "        total = sum(self.counts[context].values())\n",
        "\n",
        "        dist = []\n",
        "        vocab = []\n",
        "\n",
        "        # Create a probability distribution for each conditioning context, over the vocab that we've observed it with.\n",
        "        for idx, word in enumerate(self.counts[context]):\n",
        "            prob = self.counts[context][word]/total\n",
        "            dist.append(prob)\n",
        "            vocab.append(word)\n",
        "\n",
        "        index = np.argmax(np.random.multinomial(1, pvals=dist))\n",
        "        return vocab[index]\n",
        "\n",
        "    def generate_sequence(self, keep_ends=True):\n",
        "        generated = [\"[START]\"] * (self.order)\n",
        "        word = None\n",
        "        while word != \"[END]\":\n",
        "            context = ' '.join(generated[-self.order:] if self.order > 0 else \"\")\n",
        "            word = self.sample(context)\n",
        "            generated.append(word)\n",
        "        if not keep_ends:\n",
        "            generated = generated[self.order:-1]\n",
        "        return \" \".join(generated)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0384b034",
      "metadata": {
        "id": "0384b034"
      },
      "source": [
        "Let's create some language models of different orders from *Pride and Prejudice*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a4502623",
      "metadata": {
        "id": "a4502623",
        "outputId": "a97fc665-0775-44a1-e003-e4b931b38d46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chapter', '1', 'It', 'is', 'a', 'truth', 'universally', 'acknowledged', ',', 'that', 'a', 'single', 'man', 'in', 'possession', 'of', 'a', 'good', 'fortune', ',', 'must', 'be', 'in', 'want', 'of', 'a', 'wife', '.', '[END]']\n",
            "Context:                                                    Next: Chapter\n",
            "Context:                                                    Next: 1\n",
            "Context:                                                    Next: It\n",
            "Context:                                                    Next: is\n",
            "Context:                                                    Next: a\n",
            "Context:                                                    Next: truth\n",
            "Context:                                                    Next: universally\n",
            "Context:                                                    Next: acknowledged\n",
            "Context:                                                    Next: ,\n",
            "Context:                                                    Next: that\n",
            "Context:                                                    Next: a\n",
            "Context:                                                    Next: single\n",
            "Context:                                                    Next: man\n",
            "Context:                                                    Next: in\n",
            "Context:                                                    Next: possession\n",
            "Context:                                                    Next: of\n",
            "Context:                                                    Next: a\n",
            "Context:                                                    Next: good\n",
            "Context:                                                    Next: fortune\n",
            "Context:                                                    Next: ,\n",
            "Context:                                                    Next: must\n",
            "Context:                                                    Next: be\n",
            "Context:                                                    Next: in\n",
            "Context:                                                    Next: want\n",
            "Context:                                                    Next: of\n",
            "Context:                                                    Next: a\n",
            "Context:                                                    Next: wife\n",
            "Context:                                                    Next: .\n",
            "Context:                                                    Next: [END]\n"
          ]
        }
      ],
      "source": [
        "ngram0 = NgramModel(sequences[:max_sequences], order=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "81a51b9e",
      "metadata": {
        "id": "81a51b9e",
        "outputId": "eeff025f-93ff-4e1b-87df-ff708764e1e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[START]', 'Chapter', '1', 'It', 'is', 'a', 'truth', 'universally', 'acknowledged', ',', 'that', 'a', 'single', 'man', 'in', 'possession', 'of', 'a', 'good', 'fortune', ',', 'must', 'be', 'in', 'want', 'of', 'a', 'wife', '.', '[END]']\n",
            "Context: [START]                                            Next: Chapter\n",
            "Context: Chapter                                            Next: 1\n",
            "Context: 1                                                  Next: It\n",
            "Context: It                                                 Next: is\n",
            "Context: is                                                 Next: a\n",
            "Context: a                                                  Next: truth\n",
            "Context: truth                                              Next: universally\n",
            "Context: universally                                        Next: acknowledged\n",
            "Context: acknowledged                                       Next: ,\n",
            "Context: ,                                                  Next: that\n",
            "Context: that                                               Next: a\n",
            "Context: a                                                  Next: single\n",
            "Context: single                                             Next: man\n",
            "Context: man                                                Next: in\n",
            "Context: in                                                 Next: possession\n",
            "Context: possession                                         Next: of\n",
            "Context: of                                                 Next: a\n",
            "Context: a                                                  Next: good\n",
            "Context: good                                               Next: fortune\n",
            "Context: fortune                                            Next: ,\n",
            "Context: ,                                                  Next: must\n",
            "Context: must                                               Next: be\n",
            "Context: be                                                 Next: in\n",
            "Context: in                                                 Next: want\n",
            "Context: want                                               Next: of\n",
            "Context: of                                                 Next: a\n",
            "Context: a                                                  Next: wife\n",
            "Context: wife                                               Next: .\n",
            "Context: .                                                  Next: [END]\n"
          ]
        }
      ],
      "source": [
        "ngram1 = NgramModel(sequences[:max_sequences], order=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "35ee1ead",
      "metadata": {
        "id": "35ee1ead",
        "outputId": "e7440977-e96f-479a-d665-bdb8801fab7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[START]', '[START]', 'Chapter', '1', 'It', 'is', 'a', 'truth', 'universally', 'acknowledged', ',', 'that', 'a', 'single', 'man', 'in', 'possession', 'of', 'a', 'good', 'fortune', ',', 'must', 'be', 'in', 'want', 'of', 'a', 'wife', '.', '[END]']\n",
            "Context: [START] [START]                                    Next: Chapter\n",
            "Context: [START] Chapter                                    Next: 1\n",
            "Context: Chapter 1                                          Next: It\n",
            "Context: 1 It                                               Next: is\n",
            "Context: It is                                              Next: a\n",
            "Context: is a                                               Next: truth\n",
            "Context: a truth                                            Next: universally\n",
            "Context: truth universally                                  Next: acknowledged\n",
            "Context: universally acknowledged                           Next: ,\n",
            "Context: acknowledged ,                                     Next: that\n",
            "Context: , that                                             Next: a\n",
            "Context: that a                                             Next: single\n",
            "Context: a single                                           Next: man\n",
            "Context: single man                                         Next: in\n",
            "Context: man in                                             Next: possession\n",
            "Context: in possession                                      Next: of\n",
            "Context: possession of                                      Next: a\n",
            "Context: of a                                               Next: good\n",
            "Context: a good                                             Next: fortune\n",
            "Context: good fortune                                       Next: ,\n",
            "Context: fortune ,                                          Next: must\n",
            "Context: , must                                             Next: be\n",
            "Context: must be                                            Next: in\n",
            "Context: be in                                              Next: want\n",
            "Context: in want                                            Next: of\n",
            "Context: want of                                            Next: a\n",
            "Context: of a                                               Next: wife\n",
            "Context: a wife                                             Next: .\n",
            "Context: wife .                                             Next: [END]\n"
          ]
        }
      ],
      "source": [
        "ngram2 = NgramModel(sequences[:max_sequences], order=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c55ac8d",
      "metadata": {
        "id": "7c55ac8d"
      },
      "source": [
        "**Q1.** Create a `perplexity` function that can takes two arguments: a.) a model of *any* ngram order (from the class above); and b.) a sequence to calculate perplexity for.  You'll recall from class that perplexity under a particular language model for sequence $w$ is given by the following equation:\n",
        "\n",
        "$$\n",
        "\\textrm{perplexity}_{model}(w) = \\exp\\left(-{1 \\over N} \\sum_{i=1}^N \\log P_{model}(w_i) \\right)\n",
        "$$\n",
        "\n",
        "$P_{model}(w_i)$ calculates the probability of token $w_i$ using whatever assumptions that model makes -- for a bigram model (order 1), this is $P(w_i \\mid w_{i-1})$, for a trigram model (order 2), this is $P(w_i \\mid w_{i-2}, w_{i-1})$, etc.  Two things to note:\n",
        "\n",
        "* When calculating the probability of the first word(s), be sure to get the conditioning context right.  The conditioning context for the first word in a trigram model, for example, is $P(w_i \\mid$ [START] [START]$)$.\n",
        "* Perplexity is only calculated for the words in the actual sequence.  We don't include the $P$([START]) or $P$([END]) in the perplexity calculuation.\n",
        "\n",
        "\n",
        "*Hint*: when working on this function, you might want to debug by printing out the probabilities of each $w_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1d32301c",
      "metadata": {
        "id": "1d32301c"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "import numpy as np  # New Line: we'll need numpy for exp at the end\n",
        "\n",
        "def perplexity(model, tokens):\n",
        "    #add [START] and [END] tokens like the model expects\n",
        "    seq = [\"[START]\"] * model.order + tokens + [\"[END]\"]\n",
        "    log_prob = 0.0\n",
        "    N = len(tokens) + 1\n",
        "    for i in range(model.order, len(seq)):\n",
        "        context = \" \".join(seq[i - model.order:i])\n",
        "        word = seq[i]\n",
        "        total_count = sum(model.counts.get(context, {}).values())\n",
        "        word_count = model.counts.get(context, {}).get(word, 0) #get how many times this word followed the context\n",
        "\n",
        "        #Smoothing: if unseen, use a tiny probability instead of 0, cause previously got an error when trying to run Q3\n",
        "        if total_count == 0 or word_count == 0:\n",
        "            prob = 1e-6\n",
        "        else:\n",
        "            prob = word_count / total_count\n",
        "        log_prob += log(prob) #add log(prob) to total log probability\n",
        "\n",
        "\n",
        "    avg_log_prob = - (log_prob / N) #compute average negative log probability\n",
        "    return np.exp(avg_log_prob)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea8d0a8a",
      "metadata": {
        "id": "ea8d0a8a"
      },
      "source": [
        "**Q2**. Execute that perplexity function for the following language models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2a4b0e8a",
      "metadata": {
        "id": "2a4b0e8a",
        "outputId": "b737bf34-b006-4db1-ca52-0d8cef5ce80e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(167.8270499710048)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "perplexity(ngram0, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "dd2d4b64",
      "metadata": {
        "id": "dd2d4b64",
        "outputId": "7ff35180-2434-428d-f1c8-ce8908b6dbd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(17.1873731659743)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "perplexity(ngram1, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "05be627d",
      "metadata": {
        "id": "05be627d",
        "outputId": "f8012d8c-2374-4187-c358-6a5fefa72534",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(8.240572114787614)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "perplexity(ngram2, word_tokenize(\"She was a great friend of Mr. Bingley.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a331078",
      "metadata": {
        "id": "8a331078"
      },
      "source": [
        "**Q3.** What is the perplexity of \"She was a really great friend of Mr. Bingley.\" in the trigram language model trained above?\n",
        "\n",
        "Explain in 100 words what behavior is expected (and correct) given how an n-gram language model works and the data we are training it on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c0a4f81c",
      "metadata": {
        "id": "c0a4f81c",
        "outputId": "f21a3412-15ef-4910-b8bd-a9cdeae54219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(154.2011240737594)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "perplexity(ngram2, word_tokenize(\"She was a really great friend of Mr. Bingley.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Behaviour:** The perplexity of the trigram model is expected to be higher for “She was a really great friend of Mr. Bingley.” than for the earlier sentence without “really”. This happens because n-gram models estimate probabilities based only on counts of observed contexts in the training text.\n",
        "\n",
        "If a context like “was a really” or “a really great” is rare or unseen in Pride and Prejudice, the model assigns it very low probability. This drives up perplexity. This behavior is correct, since n-gram models cannot generalize beyond patterns they have explicitly observed."
      ],
      "metadata": {
        "id": "xnn8j0pmJBVD"
      },
      "id": "xnn8j0pmJBVD"
    },
    {
      "cell_type": "markdown",
      "id": "19d3de75-a37c-4983-8f40-a8306142caca",
      "metadata": {
        "id": "19d3de75-a37c-4983-8f40-a8306142caca"
      },
      "source": [
        "**Q4.** What 1-token sequence yields the lowest perplexity for the 0-order n-gram model? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a7143c48-5c08-4ada-af35-8488b0de3ffd",
      "metadata": {
        "id": "a7143c48-5c08-4ada-af35-8488b0de3ffd",
        "outputId": "131dce0a-a8e7-4404-edf3-ef02efc48c1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(5613.563566126232)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "perplexity(ngram0, word_tokenize(\"YOUR_ANSWER_HERE\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 1-token sequence with the lowest perplexity under the 0-order model is simply the most frequent token in the training corpus (often a comma \",\" with NLTK tokenization, otherwise a very common word like \"the\")."
      ],
      "metadata": {
        "id": "xTe0D9XwJWC0"
      },
      "id": "xTe0D9XwJWC0"
    },
    {
      "cell_type": "markdown",
      "id": "c4e059ed-8d4f-4698-b0c2-c42231a2105a",
      "metadata": {
        "id": "c4e059ed-8d4f-4698-b0c2-c42231a2105a"
      },
      "source": [
        "**Q5.** Write a function to find the n-token sequence with the lowest perplexity given an n-gram model. Explain why it should work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ffb4d492-8c34-47f9-bf3f-b03ca2dfcdd3",
      "metadata": {
        "id": "ffb4d492-8c34-47f9-bf3f-b03ca2dfcdd3"
      },
      "outputs": [],
      "source": [
        "def min_ppl_sequence(model, n=5):\n",
        "    \"\"\"Return an n-token long string that yields the lowest possible perplexity under the provided model.\"\"\"\n",
        "\n",
        "    all_tokens = [tok for context in model.counts for tok in model.counts[context]] #create a list of all tokens from training data\n",
        "\n",
        "    best_seq = None\n",
        "    best_pp = float(\"inf\")  #start with a very large value\n",
        "\n",
        "    for i in range(len(all_tokens) - n + 1): #go through every possible n-length slice of tokens\n",
        "        candidate = all_tokens[i:i+n]\n",
        "        pp = perplexity(model, candidate)\n",
        "\n",
        "        if pp < best_pp: #if this sequence has lower perplexity, remember it\n",
        "            best_pp = pp\n",
        "            best_seq = candidate\n",
        "\n",
        "    return best_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function works because - perplexity is lowest when the model assigns the highest probability to a sequence. The n-gram model estimates probabilities from observed counts in the training data.\n",
        "\n",
        "By sliding over all possible n-token sequences and calculating perplexity, the function identifies the one that is most predictable according to the model. This is expected behavior, since the model is built to favor sequences it has seen often in the corpus.\n",
        "\n",
        "As a result, the lowest perplexity sequence will usually be a very common phrase, which the model can predict with high confidence."
      ],
      "metadata": {
        "id": "7Q2IDkNgKyJg"
      },
      "id": "7Q2IDkNgKyJg"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3facf28b-323e-4f21-b736-24d707d10f6f",
      "metadata": {
        "id": "3facf28b-323e-4f21-b736-24d707d10f6f",
        "outputId": "7aae6891-8a85-41d0-df58-8777277dfd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mr.', 'Wickham', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "min_ppl_sequence(ngram1, n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f7c13d61-547c-427f-9f0e-bee7c8d27f59",
      "metadata": {
        "id": "f7c13d61-547c-427f-9f0e-bee7c8d27f59",
        "outputId": "0577ef78-3349-4048-f946-67d3f17ef3e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(8.625559780870997)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "perplexity(ngram1, min_ppl_sequence(ngram1, n=3))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}